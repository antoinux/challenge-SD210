{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fname = '../data/train.csv'\n",
    "test_fname = '../data/test.csv'\n",
    "df = pd.read_csv(train_fname, sep=';')\n",
    "df_test = pd.read_csv(test_fname, sep=';')\n",
    "(n_samples,n_variables) = df.shape\n",
    "f_con = ['APP_NB','APP_NB_PAYS','APP_NB_TYPE','NB_CLASSES','NB_ROOT_CLASSES','NB_SECTORS','NB_FIELDS','INV_NB',\n",
    "        'INV_NB_PAYS','INV_NB_TYPE','cited_n','cited_age_min','cited_age_median','cited_age_max','cited_age_mean',\n",
    "        'cited_age_std','NB_BACKWARD_NPL','NB_BACKWARD_XY','NB_BACKWARD_I','NB_BACKWARD_AUTRE','NB_BACKWARD_PL',\n",
    "        'NB_BACKWARD','pct_NB_IPC','pct_NB_IPC_LY','oecd_NB_ROOT_CLASSES','oecd_NB_BACKWARD_PL','oecd_NB_BACKWARD_NPL',\n",
    "        'IDX_ORIGIN','IDX_RADIC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.reindex(np.random.permutation(df.index));\n",
    "X_train = df[f_con].values\n",
    "y_train = df.VARIABLE_CIBLE == 'GRANTED'\n",
    "X_test = df_test[f_con].values\n",
    "imputer = Imputer()\n",
    "# Imputer permet de combler les trous quand des données manquent. Par défaut il prend la moyenne de la dernière donnée vue\n",
    "# et de la prochaine. Ces données ne sont donc pas normalisées.\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.fit_transform(X_test)\n",
    "# Normalisation des features.\n",
    "scale(X_train,copy=False);\n",
    "scale(X_test,copy=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taken_samples = n_samples\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit_transform(X_train[0:taken_samples],y_train[0:taken_samples]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (optimiste) sur le train : 0.591343414477\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = logreg.predict_proba(X_train)[:, 1]\n",
    "print('Score (optimiste) sur le train : %s' % roc_auc_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Application au test set et sauvegarde de la soumission correspondante.\n",
    "y_pred = logreg.predict_proba(X_test)[:, 1]\n",
    "np.savetxt('../subs/continue_logreg.txt', y_pred, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Utilisation de la cross-validation pour choisir l'optimisation et le coefficient de régularisation inverse C.\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "nb_thread = 4\n",
    "taken_samples = n_samples\n",
    "logreg = LogisticRegression()\n",
    "parameters = {'C':[0.0001,0.001,0.01, 0.1,1., 10., 50., 100., 500., 1000.],'solver':['liblinear','newton-cg','lbfgs']}\n",
    "def scorer(estimator, X, y):\n",
    "    return roc_auc_score(y, estimator.predict_proba(X)[:,1])\n",
    "clf = GridSearchCV(logreg, parameters, scorer, n_jobs = nb_thread)\n",
    "clf.fit(X_train[0:taken_samples],y_train[0:taken_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "liblinear\n"
     ]
    }
   ],
   "source": [
    "C = clf.best_estimator_.C\n",
    "solver = clf.best_estimator_.solver\n",
    "print(C)\n",
    "print(solver)\n",
    "taken_samples = n_samples\n",
    "logreg = LogisticRegression(C=C,solver=solver)\n",
    "logreg.fit_transform(X_train[0:taken_samples],y_train[0:taken_samples]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (optimiste) sur le train : 0.591341662854\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = logreg.predict_proba(X_train)[:, 1]\n",
    "print('Score (optimiste) sur le train : %s' % roc_auc_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion : Le score avec cross-validation est très légérement inférieur au score sans, ce qui est probablement dü à des erreurs numériques. On voit aussi que la CV a choisit presque les mêmes paramètres que les paramètres par défaut... Ce score n'étant pas satisfaisant, il faut se mettre à la recherche d'autre chose !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
